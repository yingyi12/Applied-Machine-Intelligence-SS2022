{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618370ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "930a11bd590ef11373e28b20dbd33e2e",
     "grade": false,
     "grade_id": "cell-5b50a6b3a34d5b7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*This is just a ***lecture*** notebook - you do not have to hand this in!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bfe04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbd46754f18f4424cc98761e19c1b34a",
     "grade": false,
     "grade_id": "cell-a6d92f8b18055bc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lecture 08 - 21.06.2022\n",
    "Comparing Multi-Layer-Perceptrons and Convolutional Neural Networks on  the famous MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c6c04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb35e29549ab1a6905565a78c2d7a8a",
     "grade": false,
     "grade_id": "cell-e03702c21a58c20a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.32)\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "\n",
    "tf.random.set_seed(89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef2ef3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e1e60990e4925e5967e9a1700992474",
     "grade": false,
     "grade_id": "cell-9767489ae4dca192",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## MNIST Data\n",
    "For this quick demo we use the famous MNIST dataset that contains handwritten digits from 0 to 9. It has 60.000 training samples and 10.000 test samples. The images themselves are really lightweight, containing only one channel with 28x28 pixels. The small size is also the reason this dataset is perfectly suited for using it during lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96fe33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b81499c885a0173ed996ef118e2995b1",
     "grade": false,
     "grade_id": "cell-06da59a8fd00daf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test,y_test) = mnist.load_data()\n",
    "# reshape dataset to have a single channel\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "# normalize to range [0; 1]\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "# one-hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37145732",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ecfb99243609a1cc8aa9abe1bf294d8",
     "grade": false,
     "grade_id": "cell-cc6f3d018e62b873",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].imshow(X_train[i])\n",
    "    axs[i].grid(False)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332a9fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bd5197f6e62d737d08358bc94d2f041",
     "grade": false,
     "grade_id": "cell-e4c2eab9e315243a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CNN\n",
    "At first we build a simple CNN with 2 convolutional layers with a kernel size of 5 and 3, respectively. In between we use a max pooling layer with a kernel size of 2.\n",
    "\n",
    "Inspecting the model summary you can see that the model has only around 42k parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5888b96",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2703625ead313ddbf77f9919ee3dfdb",
     "grade": false,
     "grade_id": "cell-b507198ad810f3b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "cnn_model = tf.keras.models.Sequential([\n",
    "    Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "],\n",
    "    name=\"CNN\"\n",
    ")\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe939a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa408350fc199efa795b01129142f763",
     "grade": false,
     "grade_id": "cell-c893d8e9e979de6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluating the training and the performance on the test data shows, that the CNN does a pretty decent job with a test accuracy of 99.02% - even though the model is rather small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b877b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8d960b757e766377b2931f8538073af",
     "grade": false,
     "grade_id": "cell-572b5ae4c8fd4408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We already performed the training for\n",
    "\"\"\"\n",
    "# Training the model\n",
    "cnn_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "]\n",
    "                  \n",
    "cnn_history = cnn_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2, callbacks=cnn_callbacks)\n",
    "\"\"\"\n",
    "\n",
    "# You can simply load the trained model here\n",
    "cnn_model = tf.keras.models.load_model(\"cnn_model\")\n",
    "\n",
    "# And can also have a look at how the training was\n",
    "cnn_history = json.load(open(\"cnn_model_history.json\", 'r'))\n",
    "cnn_history = pd.DataFrame(cnn_history)\n",
    "print(\"Training history:\")\n",
    "display(cnn_history)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Evaluation\")\n",
    "cnn_test_performance = cnn_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"    Test loss:     {cnn_test_performance[0]}\")\n",
    "print(f\"    Test accuracy: {cnn_test_performance[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750dee1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f635f9c4cf1b758139c625e6a8102e08",
     "grade": false,
     "grade_id": "cell-5282c0227aafefe4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also plotted the training history for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdee81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af76a10824ec2ad69fbd070bd26cb485",
     "grade": false,
     "grade_id": "cell-09faec0248e9a595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " def plot_model_history(history, ax=None, metric='loss', ep_start=1, ep_stop=None, monitor='val_loss', mode='min', plttitle=None):\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    if ep_stop is None:\n",
    "        ep_stop = len(history)\n",
    "    if plttitle is None:\n",
    "        plttitle = metric[0].swapcase() + metric[1:] + ' During Training'\n",
    "    epochs=np.arange(ep_start,ep_stop+1, dtype='int')\n",
    "    sns.lineplot(x=epochs, y=history[metric][ep_start-1:ep_stop], ax=ax)\n",
    "    sns.lineplot(x=epochs, y=history['val_' + metric][ep_start-1:ep_stop], ax=ax)\n",
    "    ax.set(title=plttitle)\n",
    "    ax.set(ylabel=metric[0].swapcase() + metric[1:])\n",
    "    ax.set(xlabel='Epoch')\n",
    "    ax.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,7))\n",
    "plot_model_history(cnn_history, ax=ax[0])\n",
    "plot_model_history(cnn_history, metric='accuracy',ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29140ee3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "285a65f0e5fed17b079e7f3f56335dea",
     "grade": false,
     "grade_id": "cell-d27a169378ce500f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## MLP\n",
    "In the next step we will build a MLP model. We already provide you with a model that has almost the same amount of paramters as our CNN has. Nevertheless, it has only an accuracy on the test data of ~97%.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Adjust the MLP. Can you come up with a model that beats the CNN? Is it even possible? How many parameters do you need for it?\n",
    "\n",
    "You can adjust the model as well as the training procedure in the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "mlp_model = tf.keras.models.Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)),\n",
    "    Dense(52, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "mlp_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "mlp_history = mlp_model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2, callbacks=mlp_callbacks)\n",
    "mlp_history = pd.DataFrame(mlp_history.history)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"\\nEvaluation\")\n",
    "mlp_test_performance = mlp_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"    Test loss:     {mlp_test_performance[0]}\")\n",
    "print(f\"    Test accuracy: {mlp_test_performance[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894c2fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55b6083cbfccaf4ccddf54410f853635",
     "grade": false,
     "grade_id": "cell-15ddd8d02595c028",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,7))\n",
    "plot_model_history(mlp_history, ax=ax[0])\n",
    "plot_model_history(mlp_history, metric='accuracy',ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
